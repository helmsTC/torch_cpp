# Configuration for MaskPLS Lidar Processor with Shared Memory
# This config file is used by both C++ node and Python inference server

lidar_processor_shm:
  ros__parameters:
    # Model configuration
    model_path: ""  # Path to your converted .pt model file (from original_pt_converter.py)
    config_path: ""  # Optional: Path to MaskPLS config file
    
    # Input/Output topics
    input_topic: "/velodyne_points"
    
    # Processing parameters
    use_cuda: true  # Use GPU if available
    auto_start_server: true  # Automatically start Python inference server
    publish_markers: true  # Publish instance visualization markers
    verbose: false  # Enable verbose logging
    
    # Model parameters (KITTI defaults)
    num_classes: 20
    overlap_threshold: 0.8
    
    # Spatial boundaries (KITTI defaults)
    x_limits: [-48.0, 48.0]
    y_limits: [-48.0, 48.0] 
    z_limits: [-4.0, 1.5]
    
    # Things class IDs for KITTI
    # 1: car, 2: bicycle, 3: motorcycle, 4: truck
    # 5: other-vehicle, 6: person, 7: bicyclist, 8: motorcyclist
    things_ids: [1, 2, 3, 4, 5, 6, 7, 8]
    
    # Performance settings
    min_points_per_instance: 50  # Minimum points to consider an instance valid
    max_instances: 100  # Maximum number of instances to track
    
    # Debug settings
    timing_window_size: 100  # Number of frames for averaging timing statistics

# Alternative configuration for NuScenes dataset
lidar_processor_shm_nuscenes:
  ros__parameters:
    model_path: ""
    config_path: ""
    input_topic: "/lidar_points"
    
    use_cuda: true
    auto_start_server: true
    publish_markers: true
    verbose: false
    
    num_classes: 17
    overlap_threshold: 0.8
    
    # NuScenes has different spatial bounds
    x_limits: [-50.0, 50.0]
    y_limits: [-50.0, 50.0]
    z_limits: [-5.0, 3.0]
    
    # Things class IDs for NuScenes
    # 2: bicycle, 3: bus, 4: car, 5: construction_vehicle
    # 6: motorcycle, 7: pedestrian, 9: trailer, 10: truck
    things_ids: [2, 3, 4, 5, 6, 7, 9, 10]
    
    min_points_per_instance: 50
    max_instances: 100

# Python inference server configuration
# These settings are read by the Python server if it's started separately
inference_server:
  # Maximum points to process in single batch
  max_points: 150000
  
  # Shared memory buffer sizes (in MB)
  input_buffer_mb: 25  # ~150k points * 4 floats * 4 bytes
  output_buffer_mb: 5  # ~150k points * 2 ints * 4 bytes
  
  # Inference settings
  batch_size: 1
  num_workers: 4
  
  # Performance monitoring
  log_interval: 10  # Log performance every N frames
  profile: false  # Enable profiling (saves to profile.json)
